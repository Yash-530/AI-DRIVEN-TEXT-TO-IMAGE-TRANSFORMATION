# -*- coding: utf-8 -*-
"""Consistency_Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sL80Zq4VOOvq1gk09ceRj0uKpIO3rujM
"""

# Cell 1: Check GPU
!nvidia-smi

# Cell 2: Clone the repository
!git clone https://github.com/deepinsight/insightface.git
!git clone https://github.com/redaigc/StoryMaker.git

# Cell 3: Install required dependencies
!pip install opencv-python transformers accelerate insightface diffusers
!pip install -U xformers

# Cell 4: Setup directory structure and download models
import os
from huggingface_hub import hf_hub_download
import shutil

# Create necessary directories
!mkdir -p models/buffalo_l
!mkdir -p checkpoints

# Download face encoder (buffalo_l model)
!wget -P models/buffalo_l https://github.com/deepinsight/insightface/releases/download/v0.7/buffalo_l.zip
!cd models/buffalo_l && unzip buffalo_l.zip

# Download StoryMaker model
!huggingface-cli download --resume-download RED-AIGC/StoryMaker --local-dir checkpoints --local-dir-use-symlinks False

!pip install onnxruntime

# Uninstall existing torchvision and torch
!pip uninstall -y torchvision torch

# Install torch and torchvision with CUDA support
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Cell 5: Import required libraries and setup pipeline
import diffusers
import cv2
import torch
import numpy as np
from PIL import Image
from insightface.app import FaceAnalysis
from diffusers import UniPCMultistepScheduler
import sys
sys.path.append('StoryMaker')
from pipeline_sdxl_storymaker import StableDiffusionXLStoryMakerPipeline

# Initialize face analysis
app = FaceAnalysis(name='buffalo_l', root='./models', providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])
app.prepare(ctx_id=0, det_size=(640, 640))

# Setup the model paths
face_adapter = './checkpoints/mask.bin'
image_encoder_path = 'laion/CLIP-ViT-H-14-laion2B-s32B-b79K'
base_model = 'huaquan/YamerMIX_v11'

# Initialize pipeline
pipe = StableDiffusionXLStoryMakerPipeline.from_pretrained(
    base_model,
    torch_dtype=torch.float16
)
pipe.cuda()
pipe.load_storymaker_adapter(image_encoder_path, face_adapter, scale=0.8, lora_scale=0.8)
pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)

!pip install pillow_heif

import cv2, os
import torch
import numpy as np
from PIL import Image
from pillow_heif import register_heif_opener
register_heif_opener()
import pillow_heif
pillow_heif.register_avif_opener()
from diffusers.utils import load_image
from diffusers import EulerAncestralDiscreteScheduler, UniPCMultistepScheduler

from insightface.app import FaceAnalysis
from pipeline_sdxl_storymaker import StableDiffusionXLStoryMakerPipeline

def resize_img(input_image, max_side=1280, min_side=960, size=None,
               pad_to_max_side=False, mode=Image.BILINEAR, base_pixel_number=64):

    w, h = input_image.size
    if size is not None:
        w_resize_new, h_resize_new = size
    else:
        ratio = min_side / min(h, w)
        w, h = round(ratio*w), round(ratio*h)
        ratio = max_side / max(h, w)
        input_image = input_image.resize([round(ratio*w), round(ratio*h)], mode)
        w_resize_new = (round(ratio * w) // base_pixel_number) * base_pixel_number
        h_resize_new = (round(ratio * h) // base_pixel_number) * base_pixel_number
    input_image = input_image.resize([w_resize_new, h_resize_new], mode)

    if pad_to_max_side:
        res = np.ones([max_side, max_side, 3], dtype=np.uint8) * 255
        offset_x = (max_side - w_resize_new) // 2
        offset_y = (max_side - h_resize_new) // 2
        res[offset_y:offset_y+h_resize_new, offset_x:offset_x+w_resize_new] = np.array(input_image)
        input_image = Image.fromarray(res)
    return input_image


# Load face encoder
app = FaceAnalysis(name='buffalo_l', root='./', providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])
app.prepare(ctx_id=0, det_size=(640, 640))

# Path to models
face_adapter = f'checkpoints/mask.bin'
image_encoder_path = 'laion/CLIP-ViT-H-14-laion2B-s32B-b79K'   #  from https://huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K
base_model_path = 'huaquan/YamerMIX_v11'  # from https://huggingface.co/huaquan/YamerMIX_v11

pipe = StableDiffusionXLStoryMakerPipeline.from_pretrained(
    base_model_path,
    torch_dtype=torch.float16,
)
pipe.cuda()
pipe.load_storymaker_adapter(image_encoder_path, face_adapter, scale=0.8, lora_scale=0.8)
pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)

def demo():
    prompt = "a person is taking a selfie, the person is wearing a red hat, and a volcano is in the distance"
    n_prompt = "bad quality, NSFW, low quality, ugly, disfigured, deformed"

    image = Image.open("examples/ldh.png").convert('RGB')
    mask_image = Image.open("examples/ldh_mask.png").convert('RGB')

    # image = resize_img(image)
    face_info = app.get(cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR))
    face_info = sorted(face_info, key=lambda x:(x['bbox'][2]-x['bbox'][0])*(x['bbox'][3]-x['bbox'][1]))[-1] # only use the maximum face

    generator = torch.Generator(device='cuda').manual_seed(666)
    for i in range(4):
        output = pipe(
            image=image, mask_image=mask_image, face_info=face_info,
            prompt=prompt,
            negative_prompt=n_prompt,
            ip_adapter_scale=0.8, lora_scale=0.8,
            num_inference_steps=25,
            guidance_scale=7.5,
            height=1280, width=960,
            generator=generator,
        ).images[0]
        output.save(f'/content/StoryMaker/examples/results/ldh666_{i}.jpg')

def demo_two():
    prompt = "A man and a woman are taking a selfie, and a volcano is in the distance"
    n_prompt = "bad quality, NSFW, low quality, ugly, disfigured, deformed"

    image = Image.open("/content/StoryMaker/examples/ldh.png").convert('RGB')
    mask_image = Image.open("/content/StoryMaker/examples/ldh_mask.png").convert('RGB')
    image_2 = Image.open("/content/StoryMaker/examples/tsy.png").convert('RGB')
    mask_image_2 = Image.open("/content/StoryMaker/examples/tsy_mask.png").convert('RGB')

    face_info = app.get(cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR))
    face_info = sorted(face_info, key=lambda x:(x['bbox'][2]-x['bbox'][0])*(x['bbox'][3]-x['bbox'][1]))[-1] # only use the maximum face
    face_info_2 = app.get(cv2.cvtColor(np.array(image_2), cv2.COLOR_RGB2BGR))
    face_info_2 = sorted(face_info_2, key=lambda x:(x['bbox'][2]-x['bbox'][0])*(x['bbox'][3]-x['bbox'][1]))[-1] # only use the maximum face

    generator = torch.Generator(device='cuda').manual_seed(666)
    for i in range(4):
        output = pipe(
            image=image, mask_image=mask_image,face_info=face_info,  #  first person
            image_2=image_2, mask_image_2=mask_image_2,face_info_2=face_info_2,  # second person
            prompt=prompt,
            negative_prompt=n_prompt,
            ip_adapter_scale=0.8, lora_scale=0.8,
            num_inference_steps=25,
            guidance_scale=7.5,
            height=1280, width=960,
            generator=generator,
        ).images[0]
        output.save(f'/content/StoryMaker/examples/results/ldh_tsy666_{i}.jpg')


if __name__ == "__main__":
    # single portrait generation
    demo()

    # two portrait generation
    #demo_two()

import matplotlib.pyplot as plt
from IPython.display import display

def demo():
    """
    Generates one image per line from the story, using a single reference image and mask,
    ensuring character consistency.
    """
    # Example multi-line story
    story = """
    The person is eating breakfase.
    The person in a subway.
    The person walking a dog.
    """

    # Split the story into individual lines
    story_lines = [line.strip() for line in story.strip().split('\n') if line.strip()]

    # Load the same reference image and mask
    image = Image.open("/content/StoryMaker/examples/ldh.png").convert('RGB')
    mask_image = Image.open("/content/StoryMaker/examples/ldh_mask.png").convert('RGB')

    # Extract face information once (character consistency)
    face_info = app.get(cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR))
    face_info = sorted(face_info, key=lambda x: (x['bbox'][2] - x['bbox'][0]) * (x['bbox'][3] - x['bbox'][1]))[-1]

    # Negative prompt (fixed)
    n_prompt = "bad quality, NSFW, low quality, ugly, disfigured, deformed"

    # Generator for reproducibility
    generator = torch.Generator(device='cuda').manual_seed(666)

    # Generate an image for each story line
    for idx, sentence in enumerate(story_lines):
        print(f"\nGenerating image for line {idx+1}: {sentence}")

        output = pipe(
            image=image, mask_image=mask_image, face_info=face_info,
            prompt=sentence,
            negative_prompt=n_prompt,
            ip_adapter_scale=0.8, lora_scale=0.8,
            num_inference_steps=25,
            guidance_scale=7.5,
            height=1280, width=960,
            generator=generator,
        ).images[0]

        # Save each generated image
        save_path = f'/content/StoryMaker/examples/results/story_image_single_{idx+1}.jpg'
        output.save(save_path)

        # Display the image
        plt.imshow(output)
        plt.title(f"Image {idx+1}")
        plt.axis('off')
        plt.show()

if __name__ == "__main__":
    # single portrait generation
    demo()

    # two portrait generation
    #demo_two()

"""to Calculate SSIM for Your demo() Code:"""

!pip install scikit-image

from skimage.metrics import structural_similarity as ssim
import numpy as np
from PIL import Image

# Load the reference image (resize to match generated images if needed)
reference_image = Image.open("/content/StoryMaker/examples/ldh.png").convert('RGB')
reference_image = reference_image.resize((960, 1280))  # Match generated image size
ref_np = np.array(reference_image)

# Number of generated images
num_generated = 3  # Adjust this based on your demo output

print("\n✅ SSIM Calculation Results:")

for idx in range(1, num_generated + 1):
    generated_path = f'/content/StoryMaker/examples/results/story_image_single_{idx}.jpg'
    generated_image = Image.open(generated_path).convert('RGB')
    gen_np = np.array(generated_image)

    # Compute SSIM
    ssim_score = ssim(ref_np, gen_np, channel_axis=-1)
    print(f"SSIM Score for story_image_single_{idx}.jpg: {ssim_score:.4f}")

"""Cosine Similarity Calculation Between Reference and Each Generated Image"""

import torch.nn.functional as F
import numpy as np
from PIL import Image

# Extract face embedding from the reference image
ref_image = Image.open("/content/StoryMaker/examples/ldh.png").convert('RGB')
ref_face_info = app.get(cv2.cvtColor(np.array(ref_image), cv2.COLOR_RGB2BGR))
ref_face_info = sorted(ref_face_info, key=lambda x: (x['bbox'][2] - x['bbox'][0]) * (x['bbox'][3] - x['bbox'][1]))[-1]
ref_face_embed = torch.tensor(ref_face_info.normed_embedding).unsqueeze(0).cuda()

# Number of generated images
num_generated = 3  # Update based on your demo

print("\n✅ Cosine Similarity (Reference vs Each Generated Image):\n")

for idx in range(1, num_generated + 1):
    gen_img_path = f'/content/StoryMaker/examples/results/story_image_single_{idx}.jpg'
    gen_image = Image.open(gen_img_path).convert('RGB')

    # Extract face embedding from the generated image
    gen_face_info = app.get(cv2.cvtColor(np.array(gen_image), cv2.COLOR_RGB2BGR))
    if gen_face_info:
        gen_face_info = sorted(gen_face_info, key=lambda x: (x['bbox'][2] - x['bbox'][0]) * (x['bbox'][3] - x['bbox'][1]))[-1]
        gen_face_embed = torch.tensor(gen_face_info.normed_embedding).unsqueeze(0).cuda()

        # Compute cosine similarity
        similarity = F.cosine_similarity(ref_face_embed, gen_face_embed).item()
        print(f"Cosine Similarity with story_image_single_{idx}.jpg: {similarity:.4f}")
    else:
        print(f"⚠️ No face detected in generated image {idx}")

import clip
import torch
from PIL import Image

# Load CLIP model
model, preprocess = clip.load("ViT-B/32", device='cuda')

# Your story lines (should match what was passed to demo())
story_lines = [
    "The person is eating breakfast.",
    "The person is in a subway.",
    "The person is walking a dog."
]

print("\n✅ CLIP Scores for demo() Generated Images:\n")
for idx, prompt in enumerate(story_lines, start=1):
    img_path = f"/content/StoryMaker/examples/results/story_image_single_{idx}.jpg"

    # Preprocess image for CLIP
    image_input = preprocess(Image.open(img_path)).unsqueeze(0).cuda()
    text_input = clip.tokenize([prompt]).cuda()

    # Encode
    with torch.no_grad():
        image_features = model.encode_image(image_input)
        text_features = model.encode_text(text_input)

    # Cosine similarity (CLIP score)
    clip_score = torch.cosine_similarity(image_features, text_features).item()
    print(f"CLIP Score for story_image_single_{idx}.jpg (Prompt: {prompt}): {clip_score:.4f}")

import matplotlib.pyplot as plt
from IPython.display import display

def demo_two():
    prompt = "A man and a woman are taking a selfie, and a volcano is in the distance"
    n_prompt = "bad quality, NSFW, low quality, ugly, disfigured, deformed"

    image = Image.open("/content/StoryMaker/examples/ldh.png").convert('RGB')
    mask_image = Image.open("/content/StoryMaker/examples/ldh_mask.png").convert('RGB')
    image_2 = Image.open("/content/StoryMaker/examples/tsy.png").convert('RGB')
    mask_image_2 = Image.open("/content/StoryMaker/examples/tsy_mask.png").convert('RGB')

    face_info = app.get(cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR))
    face_info = sorted(face_info, key=lambda x: (x['bbox'][2]-x['bbox'][0]) * (x['bbox'][3]-x['bbox'][1]))[-1]
    face_info_2 = app.get(cv2.cvtColor(np.array(image_2), cv2.COLOR_RGB2BGR))
    face_info_2 = sorted(face_info_2, key=lambda x: (x['bbox'][2]-x['bbox'][0]) * (x['bbox'][3]-x['bbox'][1]))[-1]

    generator = torch.Generator(device='cuda').manual_seed(666)

    # Generate a single image
    output = pipe(
        image=image, mask_image=mask_image, face_info=face_info,
        image_2=image_2, mask_image_2=mask_image_2, face_info_2=face_info_2,
        prompt=prompt,
        negative_prompt=n_prompt,
        ip_adapter_scale=0.8, lora_scale=0.8,
        num_inference_steps=25,
        guidance_scale=7.5,
        height=1280, width=960,
        generator=generator,
    ).images[0]

    # Optional: Save if needed
    save_path = '/content/StoryMaker/examples/results/ldh_tsy_single.jpg'
    output.save(save_path)

    # Display the generated image inline
    plt.imshow(output)
    plt.axis('off')
    plt.show()

if __name__ == "__main__":
    demo_two()

import matplotlib.pyplot as plt
from IPython.display import display

def generate_story_images(story):
    # Split story into sentences by newlines or periods (you can adjust)
    story_lines = [line.strip() for line in story.strip().split('\n') if line.strip()]

    # Load reference images once
    image = Image.open("/content/StoryMaker/examples/ldh.png").convert('RGB')
    mask_image = Image.open("/content/StoryMaker/examples/ldh_mask.png").convert('RGB')
    image_2 = Image.open("/content/StoryMaker/examples/tsy.png").convert('RGB')
    mask_image_2 = Image.open("/content/StoryMaker/examples/tsy_mask.png").convert('RGB')

    # Process face info once
    face_info = app.get(cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR))
    face_info = sorted(face_info, key=lambda x: (x['bbox'][2]-x['bbox'][0]) * (x['bbox'][3]-x['bbox'][1]))[-1]
    face_info_2 = app.get(cv2.cvtColor(np.array(image_2), cv2.COLOR_RGB2BGR))
    face_info_2 = sorted(face_info_2, key=lambda x: (x['bbox'][2]-x['bbox'][0]) * (x['bbox'][3]-x['bbox'][1]))[-1]

    # Negative prompt
    n_prompt = "bad quality, NSFW, low quality, ugly, disfigured, deformed"

    # Fixed generator seed for reproducibility
    generator = torch.Generator(device='cuda').manual_seed(666)

    # Generate images for each sentence
    for idx, sentence in enumerate(story_lines):
        print(f"Generating image for: {sentence}")
        output = pipe(
            image=image, mask_image=mask_image, face_info=face_info,
            image_2=image_2, mask_image_2=mask_image_2, face_info_2=face_info_2,
            prompt=sentence,
            negative_prompt=n_prompt,
            ip_adapter_scale=0.8, lora_scale=0.8,
            num_inference_steps=25,
            guidance_scale=7.5,
            height=1280, width=960,
            generator=generator,
        ).images[0]

        # Optional: Save each image
        save_path = f'/content/StoryMaker/examples/results/story_image_{idx+1}.jpg'
        output.save(save_path)

        # Display inline
        plt.imshow(output)
        plt.title(f"Image {idx+1}")
        plt.axis('off')
        plt.show()

if __name__ == "__main__":
    story = """
    A boy and a girl dancing in the rain.
    And then they are going on bike.
    """
    generate_story_images(story)

if __name__ == "__main__":
    story = """
    A boy and a girl met on the train.
    And then they are walking on the street.
    After some time they are taking photos.
    """
    generate_story_images(story)

from skimage.metrics import structural_similarity as ssim
import numpy as np
from PIL import Image

# Use one reference (face) image
reference_image = Image.open("/content/StoryMaker/examples/ldh.png").convert('RGB')
reference_image = reference_image.resize((960, 1280))  # Resize to generated image size
ref_np = np.array(reference_image)

# Adjust the number based on your story sentences
num_generated = 3  # Update if story has more/less lines

print("\n✅ SSIM Calculation Results (Reference vs Generated):\n")
for idx in range(1, num_generated + 1):
    gen_path = f'/content/StoryMaker/examples/results/story_image_{idx}.jpg'
    generated_image = Image.open(gen_path).convert('RGB')
    gen_np = np.array(generated_image)

    # Compute SSIM
    ssim_score = ssim(ref_np, gen_np, channel_axis=-1)
    print(f"SSIM Score for story_image_{idx}.jpg: {ssim_score:.4f}")

import torch.nn.functional as F
import numpy as np
from PIL import Image

# Extract face embeddings of reference character 1 (ldh.png)
ref_image1 = Image.open("/content/StoryMaker/examples/ldh.png").convert('RGB')
ref_face_info1 = app.get(cv2.cvtColor(np.array(ref_image1), cv2.COLOR_RGB2BGR))
ref_face_info1 = sorted(ref_face_info1, key=lambda x: (x['bbox'][2] - x['bbox'][0]) * (x['bbox'][3] - x['bbox'][1]))[-1]
ref_face_embed1 = torch.tensor(ref_face_info1.normed_embedding).unsqueeze(0).cuda()

# Extract face embeddings of reference character 2 (tsy.png)
ref_image2 = Image.open("/content/StoryMaker/examples/tsy.png").convert('RGB')
ref_face_info2 = app.get(cv2.cvtColor(np.array(ref_image2), cv2.COLOR_RGB2BGR))
ref_face_info2 = sorted(ref_face_info2, key=lambda x: (x['bbox'][2] - x['bbox'][0]) * (x['bbox'][3] - x['bbox'][1]))[-1]
ref_face_embed2 = torch.tensor(ref_face_info2.normed_embedding).unsqueeze(0).cuda()

# Number of generated story images
num_generated = 3  # Change if your story has more or less lines

print("\n✅ Cosine Similarity (Reference Characters vs Generated Image Faces):\n")

for idx in range(1, num_generated + 1):
    gen_img_path = f'/content/StoryMaker/examples/results/story_image_{idx}.jpg'
    gen_image = Image.open(gen_img_path).convert('RGB')

    # Extract faces from the generated image (may contain both characters)
    gen_faces = app.get(cv2.cvtColor(np.array(gen_image), cv2.COLOR_RGB2BGR))

    if gen_faces and len(gen_faces) >= 2:
        # Sort and get the two largest faces
        sorted_faces = sorted(gen_faces, key=lambda x: (x['bbox'][2] - x['bbox'][0]) * (x['bbox'][3] - x['bbox'][1]), reverse=True)
        gen_face_embed1 = torch.tensor(sorted_faces[0].normed_embedding).unsqueeze(0).cuda()
        gen_face_embed2 = torch.tensor(sorted_faces[1].normed_embedding).unsqueeze(0).cuda()

        # Calculate Cosine Similarity for both characters
        similarity1 = F.cosine_similarity(ref_face_embed1, gen_face_embed1).item()
        similarity2 = F.cosine_similarity(ref_face_embed2, gen_face_embed2).item()

        print(f"Image {idx}:")
        print(f"  ✅ Character 1 Similarity (ldh.png): {similarity1:.4f}")
        print(f"  ✅ Character 2 Similarity (tsy.png): {similarity2:.4f}")
    else:
        print(f"⚠️ Faces not properly detected in generated image {idx}")

!pip install git+https://github.com/openai/CLIP.git

import clip
import torch
from PIL import Image

# Load CLIP model
model, preprocess = clip.load("ViT-B/32", device='cuda')

# Match these to your story inputs
story_lines = [
    "A boy and a girl met on the train.",
    "And then they are walking on the street.",
    "After some time they are taking photos."
]

print("\n✅ CLIP Scores for generate_story_images() Outputs:\n")
for idx, prompt in enumerate(story_lines, start=1):
    img_path = f"/content/StoryMaker/examples/results/story_image_{idx}.jpg"

    # Preprocess image for CLIP
    image_input = preprocess(Image.open(img_path)).unsqueeze(0).cuda()
    text_input = clip.tokenize([prompt]).cuda()

    # Encode
    with torch.no_grad():
        image_features = model.encode_image(image_input)
        text_features = model.encode_text(text_input)

    # Cosine similarity (CLIP score)
    clip_score = torch.cosine_similarity(image_features, text_features).item()
    print(f"CLIP Score for story_image_{idx}.jpg (Prompt: {prompt}): {clip_score:.4f}")





import matplotlib.pyplot as plt
from IPython.display import display

def generate_story_images(story):
    # Split story into sentences by newlines or periods (you can adjust)
    story_lines = [line.strip() for line in story.strip().split('\n') if line.strip()]

    # Load reference images once
    image = Image.open("/content/StoryMaker/examples/ldh.png").convert('RGB')
    mask_image = Image.open("/content/StoryMaker/examples/ldh_mask.png").convert('RGB')
    image_2 = Image.open("/content/StoryMaker/examples/tsy.png").convert('RGB')
    mask_image_2 = Image.open("/content/StoryMaker/examples/tsy_mask.png").convert('RGB')

    # Process face info once
    face_info = app.get(cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR))
    face_info = sorted(face_info, key=lambda x: (x['bbox'][2]-x['bbox'][0]) * (x['bbox'][3]-x['bbox'][1]))[-1]
    face_info_2 = app.get(cv2.cvtColor(np.array(image_2), cv2.COLOR_RGB2BGR))
    face_info_2 = sorted(face_info_2, key=lambda x: (x['bbox'][2]-x['bbox'][0]) * (x['bbox'][3]-x['bbox'][1]))[-1]

    # Negative prompt
    n_prompt = "bad quality, NSFW, low quality, ugly, disfigured, deformed"

    # Fixed generator seed for reproducibility
    generator = torch.Generator(device='cuda').manual_seed(666)

    # Generate images for each sentence
    for idx, sentence in enumerate(story_lines):
        print(f"Generating image for: {sentence}")
        output = pipe(
            image=image, mask_image=mask_image, face_info=face_info,
            image_2=image_2, mask_image_2=mask_image_2, face_info_2=face_info_2,
            prompt=sentence,
            negative_prompt=n_prompt,
            ip_adapter_scale=0.8, lora_scale=0.8,
            num_inference_steps=25,
            guidance_scale=7.5,
            height=1280, width=960,
            generator=generator,
        ).images[0]

        # Optional: Save each image
        save_path = f'/content/StoryMaker/examples/results/story_image_{idx+1}.jpg'
        output.save(save_path)

        # Display inline
        plt.imshow(output)
        plt.title(f"Image {idx+1}")
        plt.axis('off')
        plt.show()

